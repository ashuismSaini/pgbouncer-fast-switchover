diff --git a/src/server.c b/src/server.c
index c6e5efe..8390672 100644
--- a/src/server.c
+++ b/src/server.c
@@ -27,6 +27,123 @@
 
 #define ERRCODE_CANNOT_CONNECT_NOW "57P03"
 
+bool routing_traffic_to_target_complete = false;
+bool source_instance_down_before_switchover = false;
+
+/**
+ * Extracts and processes query result data from a packet into a tab-separated string.
+ *
+ * This function performs the following steps:
+ * 1. Reads the number of columns from the packet header
+ * 2. Calculates total required buffer length in first pass
+ * 3. Allocates memory for output string
+ * 4. Copies column data with tab separators in second pass
+ *
+ * The returned string format is: col1\tcol2\tcol3...\tcolN\0
+ *
+ * @param pkt       Pointer to PktHdr structure containing raw packet data
+ *                  Must contain valid column count and column data
+ *
+ * @return          On success: Pointer to newly allocated string containing tab-separated column data
+ *                  On failure: NULL (with error logged)
+ *                  Caller must free the returned string
+ *
+ * @note           Function will return NULL and log error if:
+ *                 - Cannot read column count
+ *                 - Zero columns in packet
+ *                 - Cannot read column lengths or data
+ *                 - Memory allocation fails
+ *                 - Final position doesn't match calculated length
+ *
+ * @warning        The returned string must be freed by the caller to avoid memory leaks
+ */
+
+static char *query_data(PktHdr *pkt)
+{
+	uint16_t columns;
+	uint32_t length;
+	const char *data;
+	char *output = NULL;
+	size_t total_length = 0;
+	size_t pos = 0;
+	int i;
+	struct MBuf tmp_buf;
+	const uint8_t *dummy;
+
+	/* Get number of columns */
+	if (!mbuf_get_uint16be(&pkt->data, &columns)) {
+		log_error("could not get packet column count");
+		return NULL;
+	}
+
+	if (columns == 0) {
+		log_error("zero columns in packet");
+		return NULL;
+	}
+
+	/* First pass: calculate total length needed */
+	mbuf_copy(&pkt->data, &tmp_buf);  /* Create a copy of the buffer */
+	for (i = 0; i < columns; i++) {
+		if (!mbuf_get_uint32be(&tmp_buf, &length)) {
+			log_error("could not get column %d length", i);
+			return NULL;
+		}
+
+		total_length += length;
+
+		if (!mbuf_get_bytes(&tmp_buf, length, &dummy)) {
+			log_error("could not skip column %d data", i);
+			return NULL;
+		}
+	}
+
+	/* Account for tabs between columns (columns - 1) and 1 null terminator */
+	total_length += (columns - 1) + 1;
+
+	/* Allocate buffer */
+	output = malloc(total_length);
+	if (output == NULL) {
+		log_error("malloc failed: no memory in query_data");
+		return NULL;
+	}
+
+	/* Second pass: actually fetch the data */
+	for (i = 0; i < columns; i++) {
+		if (!mbuf_get_uint32be(&pkt->data, &length)) {
+			log_error("could not get column %d length (second pass)", i);
+			free(output);
+			return NULL;
+		}
+
+		if (!mbuf_get_chars(&pkt->data, length, &data)) {
+			log_error("could not get column %d data (second pass)", i);
+			free(output);
+			return NULL;
+		}
+
+		/* Copy column data */
+		memcpy(output + pos, data, length);
+		pos += length;
+
+		/* Add separator (tab or null terminator) */
+		if (i < columns - 1) {
+			output[pos++] = '\t';
+		} else {
+			output[pos++] = '\0';  /* final null terminator */
+		}
+	}
+
+	if (pos != total_length) {
+		free(output);
+		fatal("final position (%zu) does not match calculated total_length (%zu)", pos, total_length);
+	}
+
+	log_debug("Extracted data: [%s], total_length: %zu, final_pos: %zu", output, total_length, pos);
+
+	return output;
+}
+
+
 static bool load_parameter(PgSocket *server, PktHdr *pkt, bool startup)
 {
 	const char *key, *val;
@@ -122,6 +239,13 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 	const char *msg;
 	bool res = false;
 	const uint8_t *ckey;
+	char *data = NULL;
+	char *hostname = NULL;
+	char *endpoint = NULL;
+	char *role = NULL;
+	char *port_str = NULL;
+	char *status = NULL;
+	int port_num = 0;
 
 	if (incomplete_pkt(pkt)) {
 		disconnect_server(server, true, "partial pkt in login phase");
@@ -133,12 +257,28 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 		switch (pkt->type) {
 		case PqMsg_ReadyForQuery:
 		case PqMsg_ParameterStatus:
+		case PqMsg_DataRow:
 			/* handle them below */
 			break;
 
 		case PqMsg_ErrorResponse:
 			/* log & ignore errors */
 			log_server_error("S: error while executing exec_on_query", pkt);
+			/*
+			 * require topology table to exist in the cluster if using fast switchover.
+			 */
+			if (fast_switchover && server->pool->db->recovery_query && is_cluster_endpoint(server->pool->db->host)) {
+				/*
+				 * During blue-green deployment, the fast switchover feature's topology query may fail if the cluster is not part of the deployment.
+				 * To handle this scenario, if the topology query encounters an error, PgBouncer will not crash; instead, it will disable the fast switchover feature.
+				 * Once the customer sets up the blue-green deployment and performs a RELOAD, the fast switchover feature will be re-enabled
+				 * if the topology query successfully finds the node during execution.
+				 */
+				fast_switchover = false;
+			} else if (fast_switchover) {
+				fatal("does the topology table exist?");
+			}
+
 		/* fallthrough */
 		default:	/* ignore rest */
 			sbuf_prepare_skip(sbuf, pkt->len);
@@ -152,6 +292,115 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 		disconnect_server(server, true, "unknown pkt from server");
 		break;
 
+	case PqMsg_DataRow:
+		if (fast_switchover && server->pool->db->topology_query && server->pool->initial_writer_endpoint) {
+			data = query_data(pkt);
+			if (data) {
+				char *pointer_to_data = data;
+				endpoint = strsep(&pointer_to_data, "\t");
+				role = strsep(&pointer_to_data, "\t");
+				port_str = strsep(&pointer_to_data, "\t");
+				status = strsep(&pointer_to_data, "\t");
+				/*
+				 * In a Blue-Green deployment, the port and role are retrieved using the topology query.
+				 * When PgBouncer is configured with the fast switchover feature enabled for Blue-Green deployment,
+				 * it expects valid role and port values. If valid values are not found, PgBouncer will crash.
+				 */
+				if (server->pool->db->recovery_query) {
+					if (role == NULL || port_str == NULL || status == NULL) {
+						free(data);
+						fatal("port, role or status cannot be null while using the Blue Green fast switchover, port: %s, role: %s, status: %s",
+							port_str ? port_str : "NULL", role ? role : "NULL", status ? status : "NULL");
+					}
+					/*
+					 * This check handles the scenario where the customer has configured the blue-green deployment fast switch feature
+					 * but the endpoint is not part of a blue-green deployment.
+					 * In such cases, we intentionally crash PgBouncer, as fast switchovers are not supported in other deployment topologies.
+					 */
+					if (strcmp(role, BLUE_GREEN_DEPLOYMENT_TARGET) != 0 && strcmp(role, BLUE_GREEN_DEPLOYMENT_SOURCE) != 0) {
+						fatal("blue-green deployment fast switchover is only supported for roles '%s' or '%s'. The current role for your endpoint is: %s",
+							BLUE_GREEN_DEPLOYMENT_TARGET, BLUE_GREEN_DEPLOYMENT_SOURCE, role);
+					}
+					port_num = atoi(port_str);
+					if (port_num <= 0) {
+						free(data);
+						fatal("Invalid port number: %s", port_str);
+					}
+					/*
+					 * This check handles the case where the customer has configured the blue-green deployment fast switch feature
+					 * but is using the green endpoint.
+					 * In this scenario, fast switchovers are not supported. Therefore, we update the customer with the appropriate message.
+					 */
+					if ((strcmp(role, BLUE_GREEN_DEPLOYMENT_TARGET) == 0) && (strcmp(server->pool->db->host, endpoint) == 0) && (strcmp(status, AVAILABLE) == 0)) {
+						free(data);
+						fatal("for blue green deployment green end point can not be configured to acheive fast switchover feature. Confgigured endpoint : %s", server->pool->db->host);
+					}
+				} else {
+					/*
+					 * This case handles situations where the customer has not configured the recovery query
+					 * in a blue-green deployment setup.
+					 */
+					if (role && (strcmp(role, BLUE_GREEN_DEPLOYMENT_TARGET) == 0 || strcmp(role, BLUE_GREEN_DEPLOYMENT_SOURCE) == 0)) {
+						fatal("recovery query is mandatory for the blue-green deployment fast switchover feature, but it is currently missing.");
+					}
+					port_num = server->pool->db->port;
+				}
+				log_debug("got initial data, endpoint: %s, role: %s, port: %d, status: %s", endpoint ? endpoint : "NULL",
+					role ? role : "NULL", port_num, status ? status : "NULL");
+			}
+
+			/* Make a copy of endpoint for hostname */
+			hostname = strdup(endpoint);
+			if (hostname == NULL) {
+				free(data);
+				fatal("strdup: no mem for hostname");
+			}
+			if (fast_switchover && server->pool->db->recovery_query) {
+				/*
+				 * Blue-Green Deployment Fast Switchover:
+				 * Open connections only to the specified endpoint and its green counterpart; skip all other endpoints.
+				 * Connection Pool Handling:
+				 * - For writer endpoints (e.g., used with PgBouncer), ensure a connection is established to the corresponding writer on the green cluster.
+				 * - For reader endpoints, establish a connection to the equivalent reader on the green cluster.
+				 * - Apply the same logic for custom-defined endpoints.
+				 */
+				bool should_continue = setup_connection_pool_based_on_endpoint_type(server->pool->db->host, hostname);
+				if (!should_continue) {
+					free(data);
+					free(hostname);
+					sbuf_prepare_skip(sbuf, pkt->len);
+					return true;
+				}
+			}
+
+			if (!strtok(endpoint, ".")) {
+				free(data);
+				free(hostname);
+				fatal("could not parse hostname from: %s", endpoint);
+			} else {
+				PgPool *new_pool = new_pool_from_db(server->pool->db, endpoint, hostname, port_num);
+				if (new_pool) {
+					new_pool->parent_pool = server->pool;
+					new_pool->parent_pool->global_writer = server->pool;
+					new_pool->db->topology_query = strdup(server->pool->db->topology_query);
+					if (server->pool->db->recovery_query) {
+						new_pool->db->recovery_query = strdup(server->pool->db->recovery_query);
+						if (strcmp(role, BLUE_GREEN_DEPLOYMENT_SOURCE) == 0) {
+							new_pool->db->is_source_db = true;
+						}
+					}
+					launch_new_connection(new_pool, true);
+					server->pool->num_nodes++;
+				}
+			}
+
+			free(data);
+			free(hostname);
+		}
+
+		sbuf_prepare_skip(sbuf, pkt->len);
+		return true;
+
 	case PqMsg_ErrorResponse:
 		/*
 		 * If we cannot log into the server, then we drop all clients
@@ -201,7 +450,60 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 			if (!res)
 				disconnect_server(server, false, "exec_on_connect query failed");
 			break;
+		} else if (fast_switchover && server->pool->db->topology_query && server->pool->initial_writer_endpoint) {
+			server->exec_on_connect = true;
+			slog_debug(server, "server connect ok, send topology_query: %s", server->pool->db->topology_query);
+			SEND_generic(res, server, PqMsg_Query, "s", server->pool->db->topology_query);
+			if (!res)
+				disconnect_server(server, false, "exec_on_connect query failed");
+			break;
+		} else if (fast_switchover && source_instance_down_before_switchover && server->pool->db->recovery_query) {
+			if (server->pool->db->is_source_db) {
+				/*
+				 * This scenario occurs when the source instance was down before the switchover and has now become available.
+				 * During the handle server work, we monitor the metadata status. If the metadata entry shows AVAILABLE,
+				 * it indicates that the database connection was not dropped due to the switchover.
+				 * In this case, we wait for the source instance to come back online. Once it is available,
+				 * we reset the global writer to the source instance.
+				 */
+				log_debug("Setting the global writer once the source instance comes up : db_name %s", server->pool->db->name);
+				if (!server->pool->parent_pool) {
+					server->pool->parent_pool = server->pool;
+				}
+				server->pool->parent_pool->global_writer = server->pool;
+			}
+			source_instance_down_before_switchover = false;
+		} else if (fast_switchover && server->pool->db->recovery_query && routing_traffic_to_target_complete) {
+			if (server->pool->db->is_source_db) {
+				/*
+				 * After the switchover is completed, we set 'routing_traffic_to_target_complete' to true in 'handle_server_work'.
+				 * Once DNS resolution is complete for the target instance, the new DNS becomes available for queries.
+				 * Here, we monitor the startup status of the new blue instance and the 'routing_traffic_to_target_complete' flag.
+				 * Once both conditions are met, we disable the fast switchover feature.
+				 */
+				log_debug("Once the switchover is completed for DB : %s, disabling the fast switchover and reverting PgBouncer to standard connection pooling mode.", server->pool->db->name);
+				if (!server->pool->parent_pool) {
+					server->pool->parent_pool = server->pool;
+				}
+				server->pool->parent_pool->global_writer = server->pool;
+				fast_switchover = false;
+				routing_traffic_to_target_complete = false;
+			}
+		}
+
+		if (server->pool->db->topology_query && server->pool->initial_writer_endpoint && server->pool->num_nodes < 2) {
+			if (server->pool->db->recovery_query) {
+				/*
+				 * If Blue-Green Deployment fast switchover is configured on an instance/cluster node without actually creating a Blue-Green Deployment, the metadata table will return no nodes.
+				 * In this case, PgBouncer will not crash. Instead, it will disable the fast switchover feature and revert to the standard PgBouncer behavior.
+				 */
+				fast_switchover = false;
+				log_debug("topology_query did not find at least 2 nodes to use Blue Green Deployment fast switchover for DB: '%s'. PgBouncer reverting to standard connection pooling mode.", server->pool->db->name);
+			} else {
+				fatal("topology_query did not find at least 2 nodes to use fast switchover in DB: '%s'. Is the topology table populated with entries?", server->pool->db->name);
+			}
 		}
+		server->pool->initial_writer_endpoint = false;
 
 		/* login ok */
 		slog_debug(server, "server login ok, start accepting queries");
@@ -363,6 +665,8 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 	bool async_response = false;
 	struct List *item, *tmp;
 	bool ignore_packet = false;
+	bool res = false;
+	PgPool *next_pool;
 
 	Assert(!server->pool->db->admin);
 
@@ -374,6 +678,15 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 
 	/* pooling decisions will be based on this packet */
 	case PqMsg_ReadyForQuery:
+		/*
+		 * Discard topology data without sending to the client is finished. Resume regular
+		 * client/server communication.
+		 */
+		if (fast_switchover && server->pool->db->topology_query && server->pool->collect_datarows) {
+			server->pool->collect_datarows = false;
+			sbuf_prepare_skip(sbuf, pkt->len);
+			return true;
+		}
 
 		/* if partial pkt, wait */
 		if (!mbuf_get_char(&pkt->data, &state))
@@ -461,6 +774,22 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 		break;
 
 	case PqMsg_CommandComplete:
+		/*
+		 * In the process of discarding topology data without sending to the client.
+		 */
+		if (server->pool->collect_datarows) {
+			sbuf_prepare_skip(sbuf, pkt->len);
+			return true;
+		} else if (fast_switchover && server->pool->db->recovery_query && server->pool->checking_for_new_writer && server->state != SV_TESTED) {
+			/*
+			 * This handles a corner case where the recovery query did not return any data rows for any reason.
+			 * To keep searching for the new writer, we set `checking_for_new_writer` to false for this pool,
+			 * allowing the recovery query to run again through this pool.
+			 */
+			log_debug("handle_server_work: switchover started retrying to get the state of Blue Green deployment, db_name: %s", server->pool->db->name);
+			server->pool->checking_for_new_writer = false;
+		}
+
 		/* ErrorResponse and CommandComplete show end of copy mode */
 		if (server->copy_mode) {
 			slog_debug(server, "COPY finished");
@@ -545,6 +874,53 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 	/* data packets, there will be more coming */
 	case PqMsg_CopyData:
 	case PqMsg_DataRow:
+		/*
+		 * These are the rows returned from the topology query that are discarded.
+		 */
+		if (server->pool->collect_datarows) {
+			sbuf_prepare_skip(sbuf, pkt->len);
+			return true;
+		} else if (fast_switchover && server->pool->checking_for_new_writer && server->state != SV_TESTED) {
+			char *data = query_data(pkt);
+			/*
+			 * Once the switchover begins, the recovery query will run on the target instance to retrieve the switchover status from the metadata table.
+			 * When the status changes to either SWITCHOVER_IN_POST_PROCESSING or SWITCHOVER_COMPLETED,
+			 * it indicates that the target instance is ready for accepting the writes, and we can safely update the global writer to point to the target instance.
+			 */
+			if (strcmp(data, "f") == 0 || ((strcmp(data, SWITCHOVER_IN_POST_PROCESSING) == 0) || (strcmp(data, SWITCHOVER_COMPLETED) == 0))) {
+				log_debug("handle_server_work: connected to writer (pg_is_in_recovery is '%s'): db_name %s", data, server->pool->db->name);
+
+				if (!server->pool->parent_pool) {
+					server->pool->parent_pool = server->pool;
+				}
+				server->pool->parent_pool->global_writer = server->pool;
+				// new writer has been found, so indicate that we need to refresh the topology.
+				server->pool->refresh_topology = true;
+				if (server->pool->db->recovery_query) {
+					routing_traffic_to_target_complete =  true;
+				}
+			} else if (strcmp(data, AVAILABLE) == 0) {
+				/*
+				 * This scenario occurs when the source instance becomes unavailable before the switchover.
+				 * As a result, the switchover status entry in the Blue-Green Deployment metadata table remains as AVAILABLE.
+				 * In this case, we will attempt to re-establish the closed connection and set the source_instance_down_before_switchover flag to true.
+				 */
+				log_debug("handle_server_work: source instance is down before the switchover need to open the new connection, recovery query ran on : db_name %s", server->pool->db->name);
+				statlist_for_each(item, &pool_list) {
+					next_pool = container_of(item, PgPool, head);
+					if (!next_pool->last_connect_failed) {
+						continue;
+					}
+					log_debug("launch_recheck: establishing new connection to pool: %s", next_pool->db->name);
+					launch_new_connection(next_pool, /* evict_if_needed= */ true);
+				}
+				source_instance_down_before_switchover = true;
+			} else {
+				log_debug("handle_server_work: connected to reader (pg_is_in_recovery is '%s'). db_name: %s, Must keep polling until next server.", data, server->pool->db->name);
+			}
+			server->pool->checking_for_new_writer = false;
+			free(data);
+		}
 		break;
 	}
 	server->idle_tx = idle_tx;
@@ -632,7 +1008,7 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 			}
 		}
 	} else {
-		if (server->state != SV_TESTED) {
+		if (server->state != SV_TESTED && !server->pool->db->topology_query) {
 			slog_warning(server,
 				     "got packet '%c' from server when not linked",
 				     pkt_desc(pkt));
@@ -640,6 +1016,19 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 		sbuf_prepare_skip(sbuf, pkt->len);
 	}
 
+	/*
+	 * pg_is_in_recovery() finished since we received a ReadyForQuery and refresh_topology is set.
+	 * Mark the pool as needing to collect data rows and send the topology query to the writer.
+	 */
+	if (server->pool->refresh_topology && pkt->type == 'Z') {
+		server->pool->collect_datarows = true;
+		server->pool->refresh_topology = false;
+
+		SEND_generic(res, server, PqMsg_Query, "s", server->pool->db->topology_query);
+		if (!res)
+			disconnect_server(server, false, "exec_on_connect query failed");
+		}
+
 	return true;
 }
 
@@ -754,6 +1143,7 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 	bool res = false;
 	PgSocket *server = container_of(sbuf, PgSocket, sbuf);
 	PgPool *pool = server->pool;
+	PgPool *global_writer = get_global_writer(pool);
 	PktHdr pkt;
 	char infobuf[96];
 
@@ -768,8 +1158,18 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 	case SBUF_EV_RECV_FAILED:
 		if (server->state == SV_ACTIVE_CANCEL)
 			disconnect_server(server, false, "successfully sent cancel request");
-		else
+		else {
+			if (global_writer)
+				clear_global_writer(pool);
+
+			// mark main pool as failed as well (the writer)
+			if (fast_switchover) {
+				pool->last_failed_time = get_cached_time();
+				pool->last_connect_failed = true;
+				server->pool->checking_for_new_writer = false;
+			}
 			disconnect_server(server, false, "server conn crashed?");
+		}
 		break;
 	case SBUF_EV_SEND_FAILED:
 		disconnect_client(server->link, false, "unexpected eof");
@@ -810,6 +1210,10 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 		break;
 	case SBUF_EV_CONNECT_FAILED:
 		Assert(server->state == SV_LOGIN);
+		if (fast_switchover) {
+			pool->last_failed_time = get_cached_time();
+			server->pool->checking_for_new_writer = false;
+		}
 		disconnect_server(server, false, "connect failed");
 		break;
 	case SBUF_EV_CONNECT_OK:
