diff --git a/src/server.c b/src/server.c
index c6e5efe..b2611e4 100644
--- a/src/server.c
+++ b/src/server.c
@@ -27,6 +27,123 @@
 
 #define ERRCODE_CANNOT_CONNECT_NOW "57P03"
 
+bool routing_traffic_to_target_complete = false;
+bool source_instance_down_before_switchover = false;
+
+/**
+ * Extracts and processes query result data from a packet into a tab-separated string.
+ *
+ * This function performs the following steps:
+ * 1. Reads the number of columns from the packet header
+ * 2. Calculates total required buffer length in first pass
+ * 3. Allocates memory for output string
+ * 4. Copies column data with tab separators in second pass
+ *
+ * The returned string format is: col1\tcol2\tcol3...\tcolN\0
+ *
+ * @param pkt       Pointer to PktHdr structure containing raw packet data
+ *                  Must contain valid column count and column data
+ *
+ * @return          On success: Pointer to newly allocated string containing tab-separated column data
+ *                  On failure: NULL (with error logged)
+ *                  Caller must free the returned string
+ *
+ * @note           Function will return NULL and log error if:
+ *                 - Cannot read column count
+ *                 - Zero columns in packet
+ *                 - Cannot read column lengths or data
+ *                 - Memory allocation fails
+ *                 - Final position doesn't match calculated length
+ *
+ * @warning        The returned string must be freed by the caller to avoid memory leaks
+ */
+
+static char *query_data(PktHdr *pkt)
+{
+	uint16_t columns;
+	uint32_t length;
+	const char *data;
+	char *output = NULL;
+	size_t total_length = 0;
+	size_t pos = 0;
+	int i;
+	struct MBuf tmp_buf;
+	const uint8_t *dummy;
+
+	/* Get number of columns */
+	if (!mbuf_get_uint16be(&pkt->data, &columns)) {
+		log_error("could not get packet column count");
+		return NULL;
+	}
+
+	if (columns == 0) {
+		log_error("zero columns in packet");
+		return NULL;
+	}
+
+	/* First pass: calculate total length needed */
+	mbuf_copy(&pkt->data, &tmp_buf);  /* Create a copy of the buffer */
+	for (i = 0; i < columns; i++) {
+		if (!mbuf_get_uint32be(&tmp_buf, &length)) {
+			log_error("could not get column %d length", i);
+			return NULL;
+		}
+
+		total_length += length;
+
+		if (!mbuf_get_bytes(&tmp_buf, length, &dummy)) {
+			log_error("could not skip column %d data", i);
+			return NULL;
+		}
+	}
+
+	/* Account for tabs between columns (columns - 1) and 1 null terminator */
+	total_length += (columns - 1) + 1;
+
+	/* Allocate buffer */
+	output = malloc(total_length);
+	if (output == NULL) {
+		log_error("malloc failed: no memory in query_data");
+		return NULL;
+	}
+
+	/* Second pass: actually fetch the data */
+	for (i = 0; i < columns; i++) {
+		if (!mbuf_get_uint32be(&pkt->data, &length)) {
+			log_error("could not get column %d length (second pass)", i);
+			free(output);
+			return NULL;
+		}
+
+		if (!mbuf_get_chars(&pkt->data, length, &data)) {
+			log_error("could not get column %d data (second pass)", i);
+			free(output);
+			return NULL;
+		}
+
+		/* Copy column data */
+		memcpy(output + pos, data, length);
+		pos += length;
+
+		/* Add separator (tab or null terminator) */
+		if (i < columns - 1) {
+			output[pos++] = '\t';
+		} else {
+			output[pos++] = '\0';  /* final null terminator */
+		}
+	}
+
+	if (pos != total_length) {
+		free(output);
+		fatal("final position (%zu) does not match calculated total_length (%zu)", pos, total_length);
+	}
+
+	log_debug("Extracted data: [%s], total_length: %zu, final_pos: %zu", output, total_length, pos);
+
+	return output;
+}
+
+
 static bool load_parameter(PgSocket *server, PktHdr *pkt, bool startup)
 {
 	const char *key, *val;
@@ -122,6 +239,9 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 	const char *msg;
 	bool res = false;
 	const uint8_t *ckey;
+	char *data = NULL;
+	TopologyData *topology = NULL;
+	const char *topology_parser_msg;
 
 	if (incomplete_pkt(pkt)) {
 		disconnect_server(server, true, "partial pkt in login phase");
@@ -133,12 +253,28 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 		switch (pkt->type) {
 		case PqMsg_ReadyForQuery:
 		case PqMsg_ParameterStatus:
+		case PqMsg_DataRow:
 			/* handle them below */
 			break;
 
 		case PqMsg_ErrorResponse:
 			/* log & ignore errors */
 			log_server_error("S: error while executing exec_on_query", pkt);
+			/*
+			 * require topology table to exist in the cluster if using fast switchover.
+			 */
+			if (fast_switchover && server->pool->db->recovery_query && is_cluster_endpoint(server->pool->db->host)) {
+				/*
+				 * During blue-green deployment, the fast switchover feature's topology query may fail if the cluster is not part of the deployment.
+				 * To handle this scenario, if the topology query encounters an error, PgBouncer will not crash; instead, it will disable the fast switchover feature.
+				 * Once the customer sets up the blue-green deployment and performs a RELOAD, the fast switchover feature will be re-enabled
+				 * if the topology query successfully finds the node during execution.
+				 */
+				fast_switchover = false;
+			} else if (fast_switchover) {
+				fatal("does the topology table exist?");
+			}
+
 		/* fallthrough */
 		default:	/* ignore rest */
 			sbuf_prepare_skip(sbuf, pkt->len);
@@ -152,6 +288,53 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 		disconnect_server(server, true, "unknown pkt from server");
 		break;
 
+	case PqMsg_DataRow:
+		if (fast_switchover && server->pool->db->topology_query && server->pool->initial_writer_endpoint) {
+			data = query_data(pkt);
+			if (data) {
+				/* Parse topology data from a query result. */
+				topology_parser_msg = parse_topology_data(server, data, &topology);
+				if (topology_parser_msg) {
+					free(data);
+					fatal("failed while parsing the topology data. Error msg : %s", topology_parser_msg);
+				}
+
+				if (server->pool->db->recovery_query) {
+					/*
+					 * Blue-Green Deployment Fast Switchover:
+					 * Open connections only to the specified endpoint and its green counterpart; skip all other endpoints.
+					 * Connection Pool Handling:
+					 * - For writer endpoints (e.g., used with PgBouncer), ensure a connection is established to the corresponding writer on the green cluster.
+					 * - For reader endpoints, establish a connection to the equivalent reader on the green cluster.
+					 * - Apply the same logic for custom-defined endpoints.
+					 */
+					bool should_continue = setup_connection_pool_based_on_endpoint_type(server->pool->db->host, &topology->endpoint);
+					if (!should_continue) {
+						free(data);
+						cleanup_topology(topology);
+						sbuf_prepare_skip(sbuf, pkt->len);
+						return true;
+					}
+				}
+
+				/* Set up a new connection pool based on topology data. */
+				if (setup_new_pool(server, topology) == NULL) {
+					// Handle setup failure if needed
+					log_debug("Failed to initialize new connection pool for host '%s'. Possible issue with memory allocation, hostname parsing, or connection launch.",
+						topology->endpoint);
+					cleanup_topology(topology);
+					free(data);
+					fatal("Failed to set up connection pool. Aborting to prevent inconsistent topology state.");
+
+				}
+				cleanup_topology(topology);
+				free(data);
+			}
+		}
+
+		sbuf_prepare_skip(sbuf, pkt->len);
+		return true;
+
 	case PqMsg_ErrorResponse:
 		/*
 		 * If we cannot log into the server, then we drop all clients
@@ -201,7 +384,60 @@ static bool handle_server_startup(PgSocket *server, PktHdr *pkt)
 			if (!res)
 				disconnect_server(server, false, "exec_on_connect query failed");
 			break;
+		} else if (fast_switchover && server->pool->db->topology_query && server->pool->initial_writer_endpoint) {
+			server->exec_on_connect = true;
+			slog_debug(server, "server connect ok, send topology_query: %s", server->pool->db->topology_query);
+			SEND_generic(res, server, PqMsg_Query, "s", server->pool->db->topology_query);
+			if (!res)
+				disconnect_server(server, false, "exec_on_connect query failed");
+			break;
+		} else if (fast_switchover && source_instance_down_before_switchover && server->pool->db->recovery_query) {
+			if (server->pool->db->blue_green_deployment_db_type == DEFAULT) {
+				/*
+				 * This scenario occurs when the source instance was down before the switchover and has now become available.
+				 * During the handle server work, we monitor the metadata status. If the metadata entry shows AVAILABLE,
+				 * it indicates that the database connection was not dropped due to the switchover.
+				 * In this case, we wait for the source instance to come back online. Once it is available,
+				 * we reset the global writer to the source instance.
+				 */
+				log_debug("Setting the global writer once the source instance comes up : db_name %s", server->pool->db->name);
+				if (!server->pool->parent_pool) {
+					server->pool->parent_pool = server->pool;
+				}
+				server->pool->parent_pool->global_writer = server->pool;
+			}
+			source_instance_down_before_switchover = false;
+		} else if (fast_switchover && server->pool->db->recovery_query && routing_traffic_to_target_complete) {
+			if (server->pool->db->blue_green_deployment_db_type == DEFAULT) {
+				/*
+				 * After the switchover is completed, we set 'routing_traffic_to_target_complete' to true in 'handle_server_work'.
+				 * Once DNS resolution is complete for the target instance, the new DNS becomes available for queries.
+				 * Here, we monitor the startup status of the new blue instance and the 'routing_traffic_to_target_complete' flag.
+				 * Once both conditions are met, we disable the fast switchover feature.
+				 */
+				log_debug("Once the switchover is completed for DB : %s, disabling the fast switchover and reverting PgBouncer to standard connection pooling mode.", server->pool->db->name);
+				if (!server->pool->parent_pool) {
+					server->pool->parent_pool = server->pool;
+				}
+				server->pool->parent_pool->global_writer = server->pool;
+				fast_switchover = false;
+				routing_traffic_to_target_complete = false;
+			}
+		}
+
+		if (server->pool->db->topology_query && server->pool->initial_writer_endpoint && server->pool->num_nodes < 2) {
+			if (server->pool->db->recovery_query) {
+				/*
+				 * If Blue-Green Deployment fast switchover is configured on an instance/cluster node without actually creating a Blue-Green Deployment, the metadata table will return no nodes.
+				 * In this case, PgBouncer will not crash. Instead, it will disable the fast switchover feature and revert to the standard PgBouncer behavior.
+				 */
+				fast_switchover = false;
+				log_debug("topology_query did not find at least 2 nodes to use Blue Green Deployment fast switchover for DB: '%s'. PgBouncer reverting to standard connection pooling mode.", server->pool->db->name);
+			} else {
+				fatal("topology_query did not find at least 2 nodes to use fast switchover in DB: '%s'. Is the topology table populated with entries?", server->pool->db->name);
+			}
 		}
+		server->pool->initial_writer_endpoint = false;
 
 		/* login ok */
 		slog_debug(server, "server login ok, start accepting queries");
@@ -352,6 +588,24 @@ int user_client_max_connections(PgGlobalUser *user)
 		return user->max_user_client_connections;
 }
 
+/*
+ * Returns true if global writer is lost and need to update the global writer to accept writes.
+ *
+ * data: result from one of the three queries:
+ *   In case of Multi_AZ Cluster - SELECT pg_is_in_recovery(); -> "f" or "t"
+ *   In case of Blue Green Deployment of Instance - SELECT status FROM rds_tools.show_topology('pgbouncer');
+ *   In case of Blue Green Deployment of Cluster - SELECT status FROM get_blue_green_fast_switchover_metadata(\'pgbouncer\');
+ * db_type: the database type (e.g., DEFAULT) of the current pool
+ */
+
+static bool should_update_global_writer(const char *data, enum BlueGreenDeploymentDBType db_type) {
+	return	(strcmp(data, "f") == 0) ||
+			(strcmp(data, SWITCHOVER_IN_POST_PROCESSING) == 0) ||
+			(strcmp(data, SWITCHOVER_COMPLETED) == 0) ||
+			((strcmp(data, AVAILABLE) == 0) && (db_type == DEFAULT));
+}
+
+
 /* process packets on logged in connection */
 static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 {
@@ -363,6 +617,8 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 	bool async_response = false;
 	struct List *item, *tmp;
 	bool ignore_packet = false;
+	bool res = false;
+	PgPool *next_pool;
 
 	Assert(!server->pool->db->admin);
 
@@ -374,6 +630,15 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 
 	/* pooling decisions will be based on this packet */
 	case PqMsg_ReadyForQuery:
+		/*
+		 * Discard topology data without sending to the client is finished. Resume regular
+		 * client/server communication.
+		 */
+		if (fast_switchover && server->pool->db->topology_query && server->pool->collect_datarows) {
+			server->pool->collect_datarows = false;
+			sbuf_prepare_skip(sbuf, pkt->len);
+			return true;
+		}
 
 		/* if partial pkt, wait */
 		if (!mbuf_get_char(&pkt->data, &state))
@@ -461,6 +726,22 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 		break;
 
 	case PqMsg_CommandComplete:
+		/*
+		 * In the process of discarding topology data without sending to the client.
+		 */
+		if (server->pool->collect_datarows) {
+			sbuf_prepare_skip(sbuf, pkt->len);
+			return true;
+		} else if (fast_switchover && server->pool->db->recovery_query && server->pool->checking_for_new_writer && server->state != SV_TESTED) {
+			/*
+			 * This handles a corner case where the recovery query did not return any data rows for any reason.
+			 * To keep searching for the new writer, we set `checking_for_new_writer` to false for this pool,
+			 * allowing the recovery query to run again through this pool.
+			 */
+			log_debug("handle_server_work: switchover started retrying to get the state of Blue Green deployment, db_name: %s", server->pool->db->name);
+			server->pool->checking_for_new_writer = false;
+		}
+
 		/* ErrorResponse and CommandComplete show end of copy mode */
 		if (server->copy_mode) {
 			slog_debug(server, "COPY finished");
@@ -545,6 +826,61 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 	/* data packets, there will be more coming */
 	case PqMsg_CopyData:
 	case PqMsg_DataRow:
+		/*
+		 * These are the rows returned from the topology query that are discarded.
+		 */
+		if (server->pool->collect_datarows) {
+			sbuf_prepare_skip(sbuf, pkt->len);
+			return true;
+		} else if (fast_switchover && server->pool->checking_for_new_writer && server->state != SV_TESTED) {
+			char *data = query_data(pkt);
+			/*
+			 * Once the switchover begins, the recovery query will run on the target instance to retrieve the switchover status from the metadata table.
+			 * When the status changes to either SWITCHOVER_IN_POST_PROCESSING or SWITCHOVER_COMPLETED,
+			 * it indicates that the target instance is ready for accepting the writes, and we can safely update the global writer to point to the target instance.
+			 *
+			 * If the recovery query ran on a DEFAULT database type, and the topology data shows the Blue-Green deployment is in an AVAILABLE state,
+			 * that means the global writer was lost because the green (target) instance went down. So reset the global writer to the default database pool (server->pool)
+			 */
+			if (should_update_global_writer(data, server->pool->db->blue_green_deployment_db_type)) {
+				log_debug("handle_server_work: connected to writer (pg_is_in_recovery is '%s'): db_name %s", data, server->pool->db->name);
+
+				if (!server->pool->parent_pool) {
+					server->pool->parent_pool = server->pool;
+				}
+				server->pool->parent_pool->global_writer = server->pool;
+				// new writer has been found, so indicate that we need to refresh the topology.
+				server->pool->refresh_topology = true;
+				if (server->pool->db->recovery_query && server->pool->db->blue_green_deployment_db_type == BLUE_GREEN_TARGET) {
+					/*
+					 * At this point, we've run the recovery query on the green (target) instance.
+					 * The metadata table shows 'SWITCHOVER_IN_POST_PROCESSING', which meanss
+					 * green instance is now ready to accept write operations.
+					 */
+					routing_traffic_to_target_complete =  true;
+				}
+			} else if (strcmp(data, AVAILABLE) == 0) {
+				/*
+				 * This scenario occurs when the source instance becomes unavailable before the switchover.
+				 * As a result, the switchover status entry in the Blue-Green Deployment metadata table remains as AVAILABLE.
+				 * In this case, we will attempt to re-establish the closed connection and set the source_instance_down_before_switchover flag to true.
+				 */
+				log_debug("handle_server_work: source instance is down before the switchover need to open the new connection, recovery query ran on : db_name %s", server->pool->db->name);
+				statlist_for_each(item, &pool_list) {
+					next_pool = container_of(item, PgPool, head);
+					if (!next_pool->last_connect_failed) {
+						continue;
+					}
+					log_debug("launch_recheck: establishing new connection to pool: %s", next_pool->db->name);
+					launch_new_connection(next_pool, /* evict_if_needed= */ true);
+				}
+				source_instance_down_before_switchover = true;
+			} else {
+				log_debug("handle_server_work: connected to reader (pg_is_in_recovery is '%s'). db_name: %s, Must keep polling until next server.", data, server->pool->db->name);
+			}
+			server->pool->checking_for_new_writer = false;
+			free(data);
+		}
 		break;
 	}
 	server->idle_tx = idle_tx;
@@ -632,7 +968,7 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 			}
 		}
 	} else {
-		if (server->state != SV_TESTED) {
+		if (server->state != SV_TESTED && !server->pool->db->topology_query) {
 			slog_warning(server,
 				     "got packet '%c' from server when not linked",
 				     pkt_desc(pkt));
@@ -640,6 +976,19 @@ static bool handle_server_work(PgSocket *server, PktHdr *pkt)
 		sbuf_prepare_skip(sbuf, pkt->len);
 	}
 
+	/*
+	 * pg_is_in_recovery() finished since we received a ReadyForQuery and refresh_topology is set.
+	 * Mark the pool as needing to collect data rows and send the topology query to the writer.
+	 */
+	if (server->pool->refresh_topology && pkt->type == 'Z') {
+		server->pool->collect_datarows = true;
+		server->pool->refresh_topology = false;
+
+		SEND_generic(res, server, PqMsg_Query, "s", server->pool->db->topology_query);
+		if (!res)
+			disconnect_server(server, false, "exec_on_connect query failed");
+		}
+
 	return true;
 }
 
@@ -754,6 +1103,7 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 	bool res = false;
 	PgSocket *server = container_of(sbuf, PgSocket, sbuf);
 	PgPool *pool = server->pool;
+	PgPool *global_writer = get_global_writer(pool);
 	PktHdr pkt;
 	char infobuf[96];
 
@@ -768,8 +1118,18 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 	case SBUF_EV_RECV_FAILED:
 		if (server->state == SV_ACTIVE_CANCEL)
 			disconnect_server(server, false, "successfully sent cancel request");
-		else
+		else {
+			if (global_writer)
+				clear_global_writer(pool);
+
+			// mark main pool as failed as well (the writer)
+			if (fast_switchover) {
+				pool->last_failed_time = get_cached_time();
+				pool->last_connect_failed = true;
+				server->pool->checking_for_new_writer = false;
+			}
 			disconnect_server(server, false, "server conn crashed?");
+		}
 		break;
 	case SBUF_EV_SEND_FAILED:
 		disconnect_client(server->link, false, "unexpected eof");
@@ -810,6 +1170,10 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 		break;
 	case SBUF_EV_CONNECT_FAILED:
 		Assert(server->state == SV_LOGIN);
+		if (fast_switchover) {
+			pool->last_failed_time = get_cached_time();
+			server->pool->checking_for_new_writer = false;
+		}
 		disconnect_server(server, false, "connect failed");
 		break;
 	case SBUF_EV_CONNECT_OK:
@@ -893,3 +1257,262 @@ bool server_proto(SBuf *sbuf, SBufEvent evtype, struct MBuf *data)
 		takeover_login_failed();
 	return res;
 }
+
+/**
+ * @brief Parse and validate topology data from a query result.
+ *
+ * This function parses the raw tab-separated data from a topology query into a
+ * dynamically allocated TopologyData structure. It performs validation checks
+ * based on the deployment configuration, particularly for Blue-Green setups.
+ *
+ * @param server        The PgSocket associated with the query context.
+ * @param data          The raw string data returned from the topology query.
+ * @param topology_out  Output parameter; on success, points to the populated TopologyData structure.
+ *
+ * @return NULL on success, or a static error message string if parsing or validation fails.
+ *
+ * @note This function performs the following steps:
+ *       1. Allocates memory for the TopologyData structure.
+ *       2. Parses the input string into endpoint, role, port, and status fields.
+ *       3. Validates the parsed data based on the server's deployment configuration.
+ *       4. Converts the port string to an integer.
+ *       5. Enforces rules specific to Blue-Green deployments.
+ *
+ * @attention The caller is responsible for:
+ *            - Logging or handling the returned error message, if any.
+ *            - Freeing the allocated TopologyData structure on success.
+ *
+ * @warning This function no longer calls fatal() internally. Instead, it returns
+ *          descriptive error strings on failure, allowing the caller to decide how
+ *          to handle errors.
+ *
+ * @see TopologyData
+ * @see cleanup_topology()
+ * @see BLUE_GREEN_DEPLOYMENT_TARGET
+ * @see BLUE_GREEN_DEPLOYMENT_SOURCE
+ */
+const char* parse_topology_data(PgSocket *server, char *data, TopologyData **topology_out) {
+    TopologyData *topology = malloc(sizeof(TopologyData));
+    if (!topology) {
+		return "malloc failed: no memory for topology data";
+	}
+
+	memset(topology, 0, sizeof(TopologyData)); // Initialize to zero
+
+	// Make a copy of data since strsep modifies it
+	char *data_copy = strdup(data);
+	if (!data_copy) {
+		cleanup_topology(topology);
+		return "strdup failed: no memory for data copy";
+
+	}
+
+	char *pointer_to_data = data_copy;
+	char *token;
+	char *port_str = NULL;
+
+	// Parse endpoint
+	token = strsep(&pointer_to_data, "\t");
+	topology->endpoint = token ? strdup(token) : NULL;
+	if (!topology->endpoint) {
+		cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+		return "strdup failed: no memory for topology->endpoint";
+	}
+
+	// Parse role
+	token = strsep(&pointer_to_data, "\t");
+	if (token) {
+		topology->role = strdup(token);
+		if (!topology->role) {
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "strdup failed: no memory for topology->role";
+		}
+	}
+
+	// Parse port
+	token = strsep(&pointer_to_data, "\t");
+	if (token) {
+		port_str = strdup(token);
+		if (!port_str) {
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "strdup failed: no memory for port_str";
+		}
+	}
+
+	// Parse status
+	token = strsep(&pointer_to_data, "\t");
+	if (token) {
+		topology->status = strdup(token);
+		if (!topology->status) {
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "strdup failed: no memory for topology->status";
+		}
+	}
+
+	if (server->pool->db->recovery_query) {
+		/*
+		 * In a Blue-Green deployment, the port and role are retrieved using the topology query.
+		 * When PgBouncer is configured with the fast switchover feature enabled for Blue-Green deployment,
+		 * it expects valid role and port values. If valid values are not found, PgBouncer will crash.
+		 */
+		if (topology->role == NULL || port_str == NULL || topology->status == NULL) {
+			log_debug("port, role and status for Blue Green fast switchover, port: %s, role: %s, status: %s",
+				port_str ? port_str : "NULL", topology->role ? topology->role : "NULL", topology->status ? topology->status : "NULL");
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "port, role or status cannot be null while using the Blue Green fast switchover";
+		}
+
+		/*
+		 * This check handles the scenario where the customer has configured the blue-green deployment fast switch feature
+		 * but the endpoint is not part of a blue-green deployment.
+		 * In such cases, we intentionally crash PgBouncer, as fast switchovers are not supported in other deployment topologies.
+		 */
+		if (strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_TARGET) != 0 &&
+			strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_SOURCE) != 0) {
+			log_debug("The current role for your endpoint is: %s", topology->role);
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "blue-green deployment fast switchover is only supported for BLUE_GREEN_DEPLOYMENT_SOURCE and BLUE_GREEN_DEPLOYMENT_TARGET roles";
+		}
+
+		topology->port_num = atoi(port_str);
+		if (topology->port_num <= 0) {
+			log_debug("port number: %s", port_str);
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "Invalid port number in topology metadata table";
+		}
+
+		/*
+		 * This check handles the case where the customer has configured the blue-green deployment fast switch feature
+		 * but is using the green endpoint.
+		 * In this scenario, fast switchovers are not supported. Therefore, we update the customer with the appropriate message.
+		 */
+		if ((strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_TARGET) == 0) &&
+			(strcmp(server->pool->db->host, topology->endpoint) == 0) &&
+			(strcmp(topology->status, AVAILABLE) == 0)) {
+			log_debug("Configured endpoint : %s", server->pool->db->host);
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "for blue green deployment target(green) end point cannot be configured to achieve fast switchover feature.";
+		}
+    } else {
+		/*
+		 * This case handles situations where the customer has not configured the recovery query
+		 * in a blue-green deployment setup.
+		 */
+		if (topology->role && (strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_TARGET) == 0 ||
+			strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_SOURCE) == 0)) {
+			cleanup_parsing_data_and_exit(topology, data_copy, port_str);
+			return "recovery query is mandatory for the blue-green deployment fast switchover feature, but it is currently missing.";
+		}
+		topology->port_num = server->pool->db->port;
+	}
+
+	log_debug("got initial data, endpoint: %s, role: %s, port: %d, status: %s",
+		topology->endpoint ? topology->endpoint : "NULL",
+		topology->role ? topology->role : "NULL",
+		topology->port_num,
+		topology->status ? topology->status : "NULL");
+
+	free(port_str);
+	free(data_copy);
+	*topology_out = topology;
+	return NULL;
+}
+
+/**
+ * @brief Set up a new connection pool based on topology data.
+ *
+ * This function creates and configures a new PgPool instance using the provided topology data.
+ *
+ * @param server The server socket associated with the current connection.
+ * @param topology A pointer to the TopologyData structure containing parsed topology information.
+ * @param hostname The hostname for the new pool, derived from the topology endpoint.
+ *
+ * @return A pointer to the newly created PgPool if successful, or NULL if an error occurred.
+ *
+ * @note This function performs the following steps:
+ *       1. Validates and parses the hostname from the topology endpoint.
+ *       2. Creates a new pool using new_pool_from_db().
+ *       3. Sets up parent pool relationships and global writer.
+ *       4. Copies topology and recovery queries from the original pool.
+ *       5. Sets the Blue-Green deployment type based on the topology role.
+ *       6. Launches a new connection for the pool.
+ *       7. Increments the node count in the server's pool.
+ *
+ * @warning This function calls fatal() if it cannot parse the hostname, which may terminate the program.
+ *
+ * @warning The function assumes that the input parameters are valid and have been properly allocated.
+ *          It's the caller's responsibility to free the TopologyData and hostname after this function returns.
+ *
+ * @see new_pool_from_db()
+ * @see launch_new_connection()
+ */
+PgPool* setup_new_pool(PgSocket *server, TopologyData *topology) {
+	/* Make a copy of endpoint for hostname */
+	char* hostname = strdup(topology->endpoint);
+	if (hostname == NULL) {
+		log_debug("strdup: no mem for hostname");
+		return NULL;
+	}
+	if (!strtok(topology->endpoint, ".")) {
+		log_debug("could not parse hostname present in topology, hostname to parse: %s", topology->endpoint);
+		free(hostname);
+		return NULL;
+	}
+    PgPool *new_pool = new_pool_from_db(server->pool->db, topology->endpoint, hostname, topology->port_num);
+    if (!new_pool) {
+		free(hostname);
+		return NULL;
+    }
+
+    new_pool->parent_pool = server->pool;
+    new_pool->parent_pool->global_writer = server->pool;
+    new_pool->db->topology_query = strdup(server->pool->db->topology_query);
+	if (new_pool->db->topology_query == NULL) {
+		log_debug("strdup: no mem for topology_query");
+		return NULL;
+	}
+
+    if (server->pool->db->recovery_query) {
+		new_pool->db->recovery_query = strdup(server->pool->db->recovery_query);
+		if (new_pool->db->recovery_query == NULL) {
+			log_debug("strdup: no mem for recovery_query");
+			return NULL;
+		}
+		if (strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_SOURCE) == 0) {
+			new_pool->db->blue_green_deployment_db_type = BLUE_GREEN_SOURCE;
+		} else if (strcmp(topology->role, BLUE_GREEN_DEPLOYMENT_TARGET) == 0) {
+			new_pool->db->blue_green_deployment_db_type = BLUE_GREEN_TARGET;
+		}
+    }
+
+    launch_new_connection(new_pool, true);
+    server->pool->num_nodes++;
+
+	free(hostname);
+	return new_pool;
+}
+
+void cleanup_parsing_data_and_exit(TopologyData *topology, char *data_copy, char *port_str) {
+	cleanup_topology(topology);
+	if (data_copy) {
+		free(data_copy);
+	}
+	if (port_str) {
+		free(port_str);
+	}
+}
+
+void cleanup_topology(TopologyData *topology) {
+	if (topology) {
+		if (topology->endpoint) {
+			free(topology->endpoint);
+		}
+		if (topology->role) {
+			free(topology->role);
+		}
+		if (topology->status) {
+			free(topology->status);
+		}
+		free(topology);
+	}
+}
\ No newline at end of file
