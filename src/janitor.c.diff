diff --git a/src/janitor.c b/src/janitor.c
index 5131ea5..0c93e5a 100644
--- a/src/janitor.c
+++ b/src/janitor.c
@@ -24,9 +24,15 @@
 
 #include <usual/slab.h>
 
+bool fast_switchover = false;
+
 /* do full maintenance 3x per second */
 static struct timeval full_maint_period = {0, USEC / 3};
 static struct event full_maint_ev;
+/* Event used to periodically check Blue-Green topology status */
+static struct event blue_green_topology_check_ev;
+/* Time interval for Blue-Green topology monitoring */
+static struct timeval blue_green_topology_monitoring_interval;
 extern bool any_user_level_server_timeout_set;
 extern bool any_user_level_client_timeout_set;
 
@@ -127,21 +133,159 @@ void resume_all(void)
 	resume_pooler();
 }
 
+static bool update_client_pool(PgSocket *client, PgPool *new_pool)
+{
+	char *username = NULL;
+	char *passwd = NULL;
+
+	if (client->pool == new_pool)
+		return true;
+
+	username = client->login_user_credentials->name;
+	passwd = client->login_user_credentials->passwd;
+	if (!set_pool(client, new_pool->db->name, username, passwd, true)) {
+		log_error("could not set pool to: %s", new_pool->db->name);
+		return false;
+	}
+
+	return true;
+}
+
+static void reset_recently_checked(void)
+{
+	struct List *item;
+	PgPool *pool;
+
+	statlist_for_each(item, &pool_list) {
+		pool = container_of(item, PgPool, head);
+		if (pool->db->admin)
+			continue;
+
+		if (!pool->db->topology_query)
+			continue;
+
+		log_debug("resetting pool: %s", pool->db->name);
+		pool->recently_checked = false;
+	}
+}
+
 /*
- * send test/reset query to server if needed
+ * send test/reset query to server if needed. If using fast switchovers,
+ * this is the entry point for finding the new writer.
  */
-static void launch_recheck(PgPool *pool)
+static void launch_recheck(PgPool *pool, PgSocket *client)
 {
-	const char *q = cf_server_check_query;
+	char *q = cf_server_check_query;
+	char *recovery_query = NULL;
 	bool need_check = true;
 	PgSocket *server;
 	bool res = true;
+	struct List *item;
+	PgPool *next_pool;
+	usec_t polling_freq_in_ms = cf_polling_frequency / 1000;
+	usec_t last_poll_time;
+	usec_t difference_in_ms;
+	usec_t now;
+	PgPool *global_writer = get_global_writer(pool);
+
+	log_debug("launch_recheck: for db: %s, global_writer? %s", pool->db->name, global_writer ? global_writer->db->name : "no global_writer");
+
+	/* Do not use cached values for any pool database when the fast switchover feature is disabled. */
+	if (is_fast_switchover_disabled_for_pool(pool)) {
+		log_debug("launch_recheck: no topology_query for this pool, so proceeding without cache");
+	} else if (global_writer) {
+		log_debug("launch_recheck: global writer is set: using cached pool: %s", global_writer->db->name);
+		update_client_pool(client, global_writer);
+	} else if (pool->last_connect_failed) {
+		bool found = false;
+		bool need_to_reconnect = true;
+		reset_time_cache();
+		now = get_cached_time();
+		log_debug("launch_recheck: need to iterate pool list");
+
+		statlist_for_each(item, &pool_list) {
+			next_pool = container_of(item, PgPool, head);
+
+			if (!next_pool->parent_pool || next_pool->parent_pool != pool) {
+				log_debug("launch_recheck: no parent pool, skipping: %s", next_pool->db->name);
+				continue;
+			}
+
+			if (next_pool->last_connect_failed) {
+				log_debug("launch_recheck: last_connect_failed, skipping: %s", next_pool->db->name);
+				continue;
+			}
+
+			need_to_reconnect = false;
+
+			if (next_pool->checking_for_new_writer) {
+				log_debug("launch_recheck: checking_for_new_writer is true for node '%s', can't run another recovery check until done.", next_pool->db->name);
+				return;
+			}
+
+			if (next_pool->recently_checked) {
+				log_debug("launch_recheck: pool was recently checked, skipping: %s", next_pool->db->name);
+				continue;
+			}
+
+			last_poll_time = next_pool->last_poll_time;
+			difference_in_ms = (now - last_poll_time) / 1000;
+			log_debug("launch_recheck: last time checked for pool %s: now: %llu last: %llu, diff: %llu, polling_freq_max: %llu", next_pool->db->name, now, last_poll_time, difference_in_ms, cf_polling_frequency/1000);
+
+			if (difference_in_ms < polling_freq_in_ms) {
+				log_debug("launch_recheck: skipping because it's too soon for pool %s (%llu ms)", next_pool->db->name, difference_in_ms);
+				continue;
+			}
+
+			log_debug("launch_recheck: found pool during iteration, setting to: %s", next_pool->db->name);
+
+			found = update_client_pool(client, next_pool);
+			if (!found)
+				return;
+
+			break;
+		}
+
+		if (need_to_reconnect && cf_recreate_disconnected_pools) {
+			log_debug("launch_recheck: all pools failed, so need to try to reconnect to parents");
+			statlist_for_each(item, &pool_list) {
+				next_pool = container_of(item, PgPool, head);
+
+				if (!next_pool->parent_pool || next_pool->parent_pool != pool) {
+					continue;
+				}
+
+				log_debug("launch_recheck: establishing new connection to pool: %s", next_pool->db->name);
+				launch_new_connection(next_pool, /* evict_if_needed= */ true);
+			}
+
+			return;
+		} else if (!found) {
+			log_debug("could not find alternate server, need to reset all pools");
+			reset_recently_checked();
+			/* drastically reduces switchover/failover time since we don't need to wait to get called again from per_loop_activate() */
+			launch_recheck(pool, client);
+
+			return;
+		} else {
+			next_pool->last_poll_time = now;
+			next_pool->recently_checked = true;
+		}
+	}
 
 	/* find clean server */
 	while (1) {
-		server = first_socket(&pool->used_server_list);
-		if (!server)
-			return;
+		server = first_socket(&client->pool->used_server_list);
+		if (!server) {
+			log_debug("launch_recheck: could not find used_server for pool: %s", client->pool->db->name);
+
+			/* if a new connection pool was created because all three nodes in the cluster are down, servers are in the idle list instead of used list */
+			server = first_socket(&client->pool->idle_server_list);
+			if (!server) {
+				client->pool->last_connect_failed = true;
+				return;
+			}
+		}
 		if (server->ready)
 			break;
 		disconnect_server(server, true, "idle server got dirty");
@@ -156,6 +300,36 @@ static void launch_recheck(PgPool *pool)
 			need_check = false;
 	}
 
+	if (fast_switchover && pool->db->topology_query) {
+		if (!global_writer) {
+			client->pool->checking_for_new_writer = true;
+			/* If the customer has configured a recovery query, use the customer-defined recovery query. */
+			if (pool->db->recovery_query) {
+				recovery_query = strdup(pool->db->recovery_query);
+			} else {
+				recovery_query = strdup("select pg_is_in_recovery()");
+			}
+			if (recovery_query == NULL) {
+				log_error("strdup: no mem for pg_is_in_recovery()");
+				return;
+			}
+			slog_debug(server, "P: checking: %s (not done polling)", recovery_query);
+			SEND_generic(res, server, PqMsg_Query, "s", recovery_query);
+			if (!res)
+				disconnect_server(server, false, "pg_is_in_recovery() query failed");
+			free(recovery_query);
+			return;
+		} else {
+			reset_recently_checked();
+			change_server_state(server, SV_TESTED);
+
+			/* reactivate paused clients that never finished logging in */
+			if (client->state == CL_WAITING_LOGIN || client->state == CL_WAITING) {
+				activate_client(client);
+			}
+		}
+	}
+
 	if (need_check) {
 		/* send test query, wait for result */
 		slog_debug(server, "P: checking: %s", q);
@@ -210,11 +384,22 @@ static void per_loop_activate(PgPool *pool)
 			--sv_tested;
 		} else if (sv_used > 0) {
 			/* ask for more connections to be tested */
-			launch_recheck(pool);
+			launch_recheck(pool, client);
 			--sv_used;
 		} else {
 			/* not enough connections */
-			launch_new_connection(pool, /* evict_if_needed= */ true);
+			log_debug("launch_new_connection because not enough connections. number pools: %d, for: %s", statlist_count(&pool_list), pool->db->name);
+
+			if (fast_switchover && pool->db->topology_query &&
+			 	(!get_global_writer(pool) || pool->last_connect_failed)) {
+				log_debug("launch_new_connection loop: going to try to use pool cache since this pool was a writer: last_connect_failed (%d)",
+						pool->last_connect_failed);
+				launch_recheck(pool, client);
+			} else {
+				log_debug("launch_new_connection loop: need to launch new connection because pool is not already a writer");
+				launch_new_connection(pool, /* evict_if_needed= */ true);
+			}
+
 			break;
 		}
 	}
@@ -306,10 +491,7 @@ static int per_loop_wait_close(PgPool *pool)
 	return count;
 }
 
-/*
- * this function is called for each event loop.
- */
-void per_loop_maint(void)
+static void loop_maint(bool initialize)
 {
 	struct List *item;
 	PgPool *pool;
@@ -318,6 +500,7 @@ void per_loop_maint(void)
 	bool partial_pause = false;
 	bool partial_wait = false;
 	bool force_suspend = false;
+	usec_t now = get_cached_time();
 
 	if (cf_pause_mode == P_SUSPEND && cf_suspend_timeout > 0) {
 		usec_t stime = get_cached_time() - g_suspend_start;
@@ -329,13 +512,40 @@ void per_loop_maint(void)
 		pool = container_of(item, PgPool, head);
 		if (pool->db->admin)
 			continue;
+
+		if (initialize) {
+			if (!pool->db->topology_query)
+				continue;
+
+			pool->initial_writer_endpoint = true;
+			log_debug("create initial pool during startup for: %s", pool->db->name);
+		} else {
+			if (fast_switchover && pool->last_connect_failed && get_global_writer(pool)) {
+				if (now - pool->last_failed_time > cf_server_failed_delay) {
+					log_debug("last connect failed: %s, so launching new connection in per_loop_maint", pool->db->name);
+					if (routing_traffic_to_target_complete && updated_target_port_num !=0 && updated_target_port_num != pool->db->port) {
+						/*
+						 * We are here because the customer changed the green instance's port before the switchover.
+						 * Since traffic has now switched to the new target (which was previously green),
+						 * we need to update the pool's port to the new value before attempting a connection.
+						 */
+						pool->db->port = updated_target_port_num;
+					}
+					launch_new_connection(pool, true);
+				}
+			}
+		}
+
 		switch (cf_pause_mode) {
 		case P_NONE:
 			if (pool->db->db_paused) {
 				partial_pause = true;
 				active_count += per_loop_pause(pool);
 			} else {
-				per_loop_activate(pool);
+				if (initialize)
+					launch_new_connection(pool, false);
+				else
+					per_loop_activate(pool);
 			}
 			break;
 		case P_PAUSE:
@@ -374,6 +584,148 @@ void per_loop_maint(void)
 		admin_wait_close_done();
 }
 
+/**
+ * @brief Executes topology query on Blue-Green source instance to monitor deployment status.
+ *
+ * This event callback function performs topology checks by:
+ * - Finding the source (blue) instance in the pool list
+ * - Running topology query on an available server connection
+ * - Using either idle or active server connections
+ *
+ * Checks are skipped if:
+ * - Fast switchover is disabled (also removes event)
+ * - A check is already running (prevents concurrent execution)
+ * - Pool has no topology or recovery query configured
+ * - Pool is not a Blue-Green source instance
+ * - No server connections are available
+ *
+ * @note Sets is_topology_check_in_progress flag before executing query
+ * @note Disconnects server if query send fails
+ */
+static void do_topology_check(evutil_socket_t sock, short flags, void *arg)
+{
+	struct List *item;
+	PgPool *pool;
+	PgSocket *server;
+	bool res = true;
+	static bool is_running = false;
+
+	if (!fast_switchover) {
+		event_del(&blue_green_topology_check_ev);
+		return;
+	}
+
+	/* Avoid running multiple checks simultaneously */
+	if (is_running)
+		return;
+
+	is_running = true;
+
+	/* Iterate through pools to find the global writer */
+	statlist_for_each(item, &pool_list) {
+		pool = container_of(item, PgPool, head);
+
+		if (!pool->db->topology_query || pool->db->admin)
+			continue;
+
+		if (pool->db->blue_green_deployment_db_type != BLUE_GREEN_SOURCE)
+			continue;
+
+		/* Found BLUE_GREEN_SOURCE pool, now get an available server */
+		server = first_socket(&pool->idle_server_list);
+		if (!server)
+			server = first_socket(&pool->used_server_list);
+
+		if (!server || !server->ready) {
+			log_debug("topology_check: no available server in pool");
+			continue;
+		}
+
+		if (server->pool->db->is_topology_check_in_progress)
+			continue;
+
+		server->pool->db->is_topology_check_in_progress = true;
+		/* Send topology query */
+		slog_debug(server, "topology_check: executing: %s", pool->db->topology_query);
+		SEND_generic(res, server, PqMsg_Query, "s", pool->db->topology_query);
+		if (!res)
+			disconnect_server(server, false, "topology query failed");
+
+		break;
+	}
+
+	is_running = false;
+}
+
+/**
+ * @brief Set up periodic topology check for Blue-Green deployment.
+ *
+ * This function initializes and schedules a periodic event to check the topology
+ * status during Blue-Green deployment. It only activates when fast switchover
+ * is enabled.
+ *
+ * The function performs the following steps:
+ * 1. Checks if fast switchover is enabled; if not, it returns immediately.
+ * 2. Sets the monitoring interval based on the configured value.
+ * 3. Creates a persistent event for periodic topology checks.
+ * 4. Schedules the event to run at the specified interval.
+ *
+ * @note The actual topology check is performed by the do_topology_check function,
+ *       which is set as the callback for the event.
+ *
+ * @see fast_switchover
+ * @see cf_topology_monitoring_interval
+ * @see do_topology_check
+ */
+
+static void topology_check_setup(void)
+{
+	// Initialize interval from runtime-configured value
+	blue_green_topology_monitoring_interval.tv_sec = cf_topology_monitoring_interval/USEC;
+	blue_green_topology_monitoring_interval.tv_usec = 0;
+
+	event_del(&blue_green_topology_check_ev);
+    /* Set up periodic event */
+    event_assign(&blue_green_topology_check_ev, pgb_event_base, -1, EV_PERSIST, do_topology_check, NULL);
+    if (event_add(&blue_green_topology_check_ev, &blue_green_topology_monitoring_interval) < 0)
+		log_warning("event_add failed for blue green topology check, errno : %s", strerror(errno));
+}
+
+/*
+ * Used to pre-create connection pools at pgbouncer init time.
+ */
+void run_once_to_init(void)
+{
+	if (!fast_switchover) {
+		log_debug("database does not have fast_switchovers enabled, so will not precreate pools to nodes");
+		return;
+	}
+
+	log_debug("creating pool for %s", fast_switchover_db->name);
+	if (fast_switchover_db && fast_switchover_db->forced_user_credentials) {
+		PgPool *pool = get_pool(fast_switchover_db, fast_switchover_db->forced_user_credentials);
+		if (!pool)
+			fatal("pool could not be created for %s", fast_switchover_db->name);
+		pool->num_nodes = 0;
+
+		if (fast_switchover_db->recovery_query)
+			topology_check_setup();
+	} else {
+		fatal("Failed to create a connection pool for database: %s. Username is missing. To enable fast switchover, "
+			   "please provide a user for this database in config.", fast_switchover_db->name);
+	}
+
+	loop_maint(true);
+}
+
+/*
+ * this function is called for each event loop.
+ */
+void per_loop_maint(void)
+{
+	loop_maint(false);
+}
+
 /* maintaining clients in pool */
 static void pool_client_maint(PgPool *pool)
 {
@@ -494,6 +846,7 @@ static void check_unused_servers(PgPool *pool, struct StatList *slist, bool idle
 		} else if (server->state == SV_USED && !server->ready) {
 			disconnect_server(server, true, "SV_USED server got dirty");
 		} else if (cf_server_idle_timeout > 0 && idle > cf_server_idle_timeout
+			   && (pool->db && is_fast_switchover_disabled_for_pool(pool))
 			   && (pool_min_pool_size(pool) == 0 || pool_connected_server_count(pool) > pool_min_pool_size(pool))) {
 			disconnect_server(server, true, "server idle timeout");
 		} else if (age >= server_lifetime) {
@@ -877,6 +1230,8 @@ void kill_database(PgDatabase *db)
 	if (db->forced_user_credentials)
 		slab_free(credentials_cache, db->forced_user_credentials);
 	free(db->connect_query);
+	free(db->topology_query);
+	free(db->recovery_query);
 	if (db->inactive_time) {
 		statlist_remove(&autodatabase_idle_list, &db->head);
 	} else {
